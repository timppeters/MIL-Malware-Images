import torch
import torch.nn as nn
from efficientnet_pytorch import EfficientNet


class CNN(nn.Module):
    def __init__(self, num_classes=2):
        super(CNN, self).__init__()
        self.num_classes = num_classes

        # Lazy layers don't require input size specification
        self.feature_extractor = nn.Sequential(
            nn.LazyConv2d(16, kernel_size=5),
            nn.ReLU(),
            nn.LazyConv2d(32, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),
            nn.LazyConv2d(64, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),
        )

        self.linear = nn.Sequential(
            nn.LazyLinear(512),
            nn.ReLU(),
        )

        self.classifier = nn.Sequential(
            nn.LazyLinear(self.num_classes)
        )

    def forward(self, x):
        x = self.feature_extractor(x)
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = self.linear(x)
        Y_prob = self.classifier(x)

        return Y_prob

class EfficientNetModel(nn.Module):
    def __init__(self, num_classes=2):
        super(EfficientNetModel, self).__init__()
        self.EN = EfficientNet.from_pretrained('efficientnet-b1')
        self.num_classes = num_classes

        self.globalpool2d = nn.Sequential(
            nn.AdaptiveAvgPool2d(1)
        )

        self.classifier = nn.Sequential(
            nn.Linear(1280, self.num_classes),
        )

    def forward(self, x):
        # H - the feature vectors
        H = self.EN.extract_features(x)
        # (n, 1280,_,_)   
        H = self.globalpool2d(H)
        # (n, 1280, 1, 1)
        H = torch.flatten(H, 1) # flatten all dimensions except batch

        Y_prob = self.classifier(H)

        return Y_prob