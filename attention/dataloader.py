"""Pytorch dataset object that loads a Malware dataset as bags of image patches."""

import math
import torch
import numpy as np
import pandas as pd
from pathlib import Path
import torch.utils.data as data_utils
from torchvision import transforms
from tqdm import tqdm
import os.path
from PIL import Image

# Base / MicrosoftBIG2015 loader
class MalwareBags(data_utils.Dataset):
    def __init__(self, bags_dir, labels_file, small_images_dir, channels = 1, transform=transforms.Compose([transforms.ToTensor()]), target_transform=None, test=False, lazy=False, adversarial=False, adversarial_type="zeros"):
        self.bags_dir = bags_dir
        self.bag_labels = pd.read_csv(labels_file)
        self.transform = transform
        self.target_transform = target_transform
        self.channels = channels
        self.lazy = lazy
        self.test = test
        self.small_images_dir = small_images_dir
        self.adversarial = adversarial
        self.adversarial_type = adversarial_type

        # If lazy loading (not pre-loading - adversarial case)
        if self.lazy:
            self.small_images, self.og = self._load_small_images()
        else:
            self.bags_list, self.small_images = self._load_bags() # pre-load images into memory

        if test:
            # List of images sorted in order of bag_labels, to make confusion matrix image matching more efficient
            self.images_sorted = [self.small_images[sample_id] for sample_id in self.bag_labels["Id"]]
            if lazy:
                self.og_images = [self.og[sample_id] for sample_id in self.bag_labels["Id"]]

    # Pre-load small images even if lazy-loading, as when num_workers > 0 each worker instance has a different small_images dict which breaks confusion matrix Comet logging
    # Also when num_workers > 0, the original dataloader won't have any small_images if lazy loading small images
    def _load_small_images(self):
        small_images = {}
        og = {}
        print("Loading small images")
        if self.test:
            for bag_id in tqdm(self.bag_labels['Id'], ascii=True):
                bin_path = Path(f'{self.bags_dir}/{bag_id}.bin')
                try:
                    bin_data = np.fromfile(bin_path, dtype=np.uint8)
                except:
                    print(f'Not available: {bag_id}')
                    continue

                if self.test:
                    og[bag_id] = bin_data.copy()

                if self.adversarial:
                    if self.adversarial_type == "zeros":
                        bin_data.resize(10000*10000)
                    else:
                        pad_data = (np.random.rand(10000*10000 - len(bin_data))*255).astype(np.uint8) # Uniform random noise 0-255
                        bin_data = np.concatenate((bin_data, pad_data))
            
                small_image = self._make_small_image(bin_data)
                if (self.small_images_dir):
                    small_image = Image.open(Path(f'{self.small_images_dir}/{bag_id}.bmp'))
                else:
                    small_image = self._make_small_image(bin_data)
                    #small_image.save(f"data_sets/BIG2015/train_images_small/{bag_id}.bmp")
                small_images[bag_id] = small_image
        print("All small images loaded")
        return small_images, og

    def _load_bags(self):
        # Check if the samples exist to avoid loading errors
        print("Checking samples exist")
        for bag_id in tqdm(self.bag_labels['Id'], ascii=True):
            bin_path = Path(f'{self.bags_dir}/{bag_id}.bin')
            if (not os.path.isfile(bin_path)):
                print(f"File doesn't exist: {bag_id}")
        
        # Load the samples as bags of images into memory
        bags_list = {}
        small_images = {}
        print("Loading bags")
        for bag_id in tqdm(self.bag_labels['Id'], ascii=True):
            bin_path = Path(f'{self.bags_dir}/{bag_id}.bin')
            try:
                bin_data = np.fromfile(bin_path, dtype=np.uint8)
            except:
                print(f'Not available: {bag_id}')
                continue

            # if self.adversarial: # never reached as adversarial dataset doesn't get pre-loaded
            #     bin_data.resize(10000*10000)
            
            bag = self._make_bag(bin_data)
            bags_list[bag_id] = bag

            if self.test:
                if (self.small_images_dir):
                    small_image = Image.open(Path(f'{self.small_images_dir}/{bag_id}.bmp'))
                else:
                    small_image = self._make_small_image(bin_data)
                    #small_image.save(f"data_sets/BIG2015/train_images_small/{bag_id}.bmp")
                small_images[bag_id] = small_image
        print("All bags loaded")
        return bags_list, small_images

    # Small images are uploaded to CometML for confusion matrix visualisation
    def _make_small_image(self, bin_data):
        # Load the malware image and resize it to the square root (square image)
        sqrt = int(math.ceil(math.sqrt(len(bin_data))))
        data = np.resize(bin_data, sqrt**2) # Pad to sqrt*sqrt
        data = data.reshape((sqrt, sqrt))
        
        # Resize the image
        img = Image.fromarray(data)
        img = img.resize((224, 224), resample=Image.BILINEAR)
        
        return img

    # Make a bag of image patches from a single large image
    def _make_bag(self, bin_data):

        # Pad the data to a multiple of 224x224
        padded_bin_data = np.pad(bin_data, (0, (math.ceil(len(bin_data)/(224**2)) * (224**2)) - len(bin_data)))

        # Reshape the image into a bag of instaces of 224px x 224px (H x W x C)
        bag = padded_bin_data.reshape(-1, 224, 224, 1)

        # 3-channel option for when using pre-trained models (H x W x 3)
        if (self.channels == 3):
            bag = np.broadcast_to(bag, bag.shape[:-1]+(3,)).copy()
        
        # Convert to tensor & scale to [0-1] & possibly normalise if using pre-trained model (N x C x H x W)
        bag = torch.stack([self.transform(instance) for instance in bag])
        return bag

    def __len__(self):
        return len(self.bag_labels)

    def __getitem__(self, index):
        label = self.bag_labels.iloc[index, 1]
        bag_id = self.bag_labels.iloc[index, 0]

        if not self.lazy:
            bag = self.bags_list[bag_id]
        else:
            bin_path = Path(f'{self.bags_dir}/{bag_id}.bin')
            bin_data = np.fromfile(bin_path, dtype=np.uint8)

            if self.adversarial:
                if self.adversarial_type == "zeros":
                    bin_data.resize(10000*10000)
                else:
                    pad_data = (np.random.rand(10000*10000 - len(bin_data))*255).astype(np.uint8) # Uniform random noise 0-255
                    bin_data = np.concatenate((bin_data, pad_data))

            bag = self._make_bag(bin_data)

            # if self.test:
            #     small_image = self._make_small_image(bin_data)
            #     # self.small_images[bag_id] = small_image
            #     self.small_images.update({bag_id:small_image})
            #     print(len(self.small_images))

        if self.target_transform:
            label = self.target_transform(label)

        return bag, label

    def get_sample_id_by_index(self, index):
        sample_id = self.bag_labels.iloc[index, 0]
        return sample_id