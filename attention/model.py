import torch
import torch.nn as nn
import torch.nn.functional as F
from efficientnet_pytorch import EfficientNet

class Attention(nn.Module):
    def __init__(self, num_classes=2):
        super(Attention, self).__init__()
        self.L = 512 # Size of feature vectors
        self.D = 128 # Size of attention vectors
        self.K = 1 # Number of attention vectors
        self.num_classes = num_classes

        self.feature_extractor_part1 = nn.Sequential(
            nn.LazyConv2d(16, kernel_size=5),
            nn.ReLU(),
            nn.LazyConv2d(32, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),
            nn.LazyConv2d(64, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),
        )

        self.feature_extractor_part2 = nn.Sequential(
            nn.LazyLinear(self.L),
            nn.ReLU(),
        )

        # Normal Attention
        self.attention = nn.Sequential(
            nn.Linear(self.L, self.D),
            nn.Tanh(),
            nn.Linear(self.D, self.K)
        )

        # Multi-class classification
        self.classifier = nn.Sequential(
            nn.Linear(self.L*self.K, self.num_classes)
        )

    def forward(self, x):
        # Technically using batch_size=1 so remove it, as using bag_size as batch_size
        x = x.squeeze(0)

        features = []
        instances_per_subbatch = 60 # All bags are less than 60 in non-adversarial setting

        for sub_batch in torch.split(x, instances_per_subbatch):
            # H - the feature vectors
            H = self.feature_extractor_part1(sub_batch)
            H = torch.flatten(H, 1) # flatten all dimensions except bag
            H = self.feature_extractor_part2(H)  # NxL

            features.append(H)
        
        # Features is now ~2000xL (for adversarial)
        features = torch.cat(features)

        # A - the attention weights
        A = self.attention(features)  # NxK
        A = torch.transpose(A, 1, 0)  # KxN
        A = F.softmax(A, dim=1)  # softmax over N

        # Select top 'n' instances
        top_n = 12 if x.size(0) > 12 else x.size(0) # Average bag size is 12.6 for microsoftBIG
        top_n_attention_values, top_n_attention_indices = A.topk(top_n, dim=1)  # Get top 'n' attention weights and their indices

        # Use indices to select corresponding instances
        top_n_features = features[top_n_attention_indices.flatten()]

        # H aggregated according to weights A, produces bag representation M
        M = torch.mm(top_n_attention_values, top_n_features)  # KxL

        Y_prob = self.classifier(M)

        return Y_prob, A

class GatedAttention(nn.Module):
    def __init__(self, num_classes=2):
        super(Attention, self).__init__()
        self.L = 512 # Size of feature vectors
        self.D = 128 # Size of attention vectors
        self.K = 1 # Number of attention vectors
        self.num_classes = num_classes

        self.feature_extractor_part1 = nn.Sequential(
            nn.LazyConv2d(16, kernel_size=5),
            nn.ReLU(),
            nn.LazyConv2d(32, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),
            nn.LazyConv2d(64, kernel_size=5, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),
        )

        self.feature_extractor_part2 = nn.Sequential(
            nn.LazyLinear(self.L),
            nn.ReLU(),
        )

        # Gated Attention
        self.attention_V = nn.Sequential(
            nn.Linear(self.L, self.D),
            nn.Tanh()
        )
        # Gated Attention
        self.attention_U = nn.Sequential(
            nn.Linear(self.L, self.D),
            nn.Sigmoid()
        )
        # Gated Attention
        self.attention_weights = nn.Linear(self.D, self.K)

        # Multi-class classification
        self.classifier = nn.Sequential(
            nn.Linear(self.L*self.K, self.num_classes)
        )

    def forward(self, x):
        # Technically using batch_size=1 so remove it, as using bag_size as batch_size
        x = x.squeeze(0)

        features = []
        instances_per_subbatch = 60 # All bags are less than 60 in non-adversarial setting

        for sub_batch in torch.split(x, instances_per_subbatch):
            # H - the feature vectors
            H = self.feature_extractor_part1(sub_batch)
            H = torch.flatten(H, 1) # flatten all dimensions except bag
            H = self.feature_extractor_part2(H)  # NxL

            features.append(H)
        
        # Features is now ~2000xL (for adversarial)
        features = torch.cat(features)

        # Gated Attention
        A_V = self.attention_V(features)  # NxD
        A_U = self.attention_U(features)  # NxD
        A = self.attention_weights(A_V * A_U) # element wise multiplication # NxK
        A = torch.transpose(A, 1, 0)  # KxN
        A = F.softmax(A, dim=1)  # softmax over N

        # Select top 'n' instances
        top_n = 12 if x.size(0) > 12 else x.size(0) # Average bag size is 12.6 for microsoftBIG
        top_n_attention_values, top_n_attention_indices = A.topk(top_n, dim=1)  # Get top 'n' attention weights and their indices

        # Use indices to select corresponding instances
        top_n_features = features[top_n_attention_indices.flatten()]

        # H aggregated according to weights A, produces bag representation M
        M = torch.mm(top_n_attention_values, top_n_features)  # KxL

        Y_prob = self.classifier(M)

        return Y_prob, A

# Just for testing
class EfficientNetMIL(nn.Module):
    def __init__(self, num_classes=2):
        super(EfficientNetMIL, self).__init__()
        self.EN = EfficientNet.from_pretrained('efficientnet-b1')
        self.L = 512 # Size of feature vectors
        self.D = 128 # Size of attention vectors
        self.K = 1 # Number of attention vectors
        self.num_classes = num_classes

        self.globalpool2d = nn.Sequential(
            nn.AdaptiveAvgPool2d(1)
        )

        self.linear = nn.Sequential(
            nn.Linear(1280, self.L),
            nn.ReLU(),
        )        

        self.attention = nn.Sequential(
            nn.Linear(self.L, self.D),
            nn.Tanh(),
            nn.Linear(self.D, self.K)
        )

        self.MIL = nn.Sequential(
            nn.AdaptiveAvgPool1d(1)
        )

        # Multi-class classification
        self.classifier = nn.Sequential(
            nn.Linear(1280, self.num_classes)
        )

    def forward(self, x):
        # Technically using batch_size=1 so remove it, as using bag_size as batch_size
        x = x.squeeze(0)

        # # H - the feature vectors
        H = self.EN.extract_features(x)
        # (n, 1280, 8, 8)  
        H = self.globalpool2d(H)
        # (n, 1280, 1, 1)
        H = torch.flatten(H, 1) # flatten all dimensions except bag
        # (n, 1280)
        H = self.linear(H)
        # (n, 512)

        # A - the attention weights
        A = self.attention(H)  # NxK
        A = torch.transpose(A, 1, 0)  # KxN
        A = F.softmax(A, dim=1)  # softmax over N
        # H aggregated according to weights A, produces bag representation M
        M = torch.mm(A, H)  # KxL

        # A = torch.Tensor()
        # #M = self.MIL()
        # M = torch.mean(H, 0, True)
        # # (1, 1280)

        Y_prob = self.classifier(M)

        return Y_prob, A
